{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import datetime \n",
    "import csv\n",
    "import configparser\n",
    "import platform\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Grayscale(),transforms.ToTensor(),])\n",
    "train_data_path = './horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "loader_train = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True, num_workers=1)\n",
    "test_data_path = './horse-or-human/validation'\n",
    "testset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "loader_test = torch.utils.data.DataLoader(valset, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        self.number_class   = num_classes\n",
    "\n",
    "        _size_image     = 100* 100\n",
    "        _num1           = 50\n",
    "        _num2           = 50\n",
    "        \n",
    "        self.fc1        = nn.Linear(_size_image, _num1, bias=True)\n",
    "        self.fc2        = nn.Linear(_num1, _num2, bias=True)\n",
    "        self.fc3        = nn.Linear(_num2, num_classes, bias=True)\n",
    "\n",
    "        self.fc_layer1  = nn.Sequential(self.fc1, nn.ReLU(True))\n",
    "        self.fc_layer2  = nn.Sequential(self.fc2, nn.ReLU(True))\n",
    "        self.fc_layer3  = nn.Sequential(self.fc3, nn.ReLU(True))\n",
    "        \n",
    "        self.classifier = nn.Sequential(self.fc_layer1, self.fc_layer2, self.fc_layer3)\n",
    "        \n",
    "        self._initialize_weight()        \n",
    "        \n",
    "    def _initialize_weight(self):        \n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                n = m.in_features \n",
    "                m.weight.data.uniform_(- 1.0 / math.sqrt(n), 1.0 / math.sqrt(n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer   = optim.SGD(model.parameters(),lr = 0.01)\n",
    "objective   = nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "\n",
    "    # print('train the model at given epoch')\n",
    "\n",
    "    loss_train          = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idx_batch, (data, target) in enumerate(loader_train):\n",
    "\n",
    "#         if bCuda:\n",
    "        \n",
    "#             data, target    = data.cuda(), target.cuda()\n",
    "\n",
    "        data, target    = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output  = model(data)\n",
    "        loss    = objective(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train_batch    = loss.item() / len(data)\n",
    "        loss_train.append(loss_train_batch)\n",
    "        \n",
    "    loss_train_mean     = np.mean(loss_train)\n",
    "    loss_train_std      = np.std(loss_train)\n",
    "    return {'loss_train_mean': loss_train_mean, 'loss_train_std': loss_train_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    # print('test the model at given epoch')\n",
    "\n",
    "    accuracy_test   = []\n",
    "    loss_test       = 0\n",
    "    correct         = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for idx_batch, (data, target) in enumerate(loader_test):\n",
    "\n",
    "\n",
    "        data, target    = Variable(data), Variable(target)\n",
    "\n",
    "        output  = model(data)\n",
    "        loss    = objective(output, target)\n",
    "\n",
    "        loss_test   += loss.item()\n",
    "        pred        = output.data.max(1)[1]\n",
    "        correct     += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    loss_test       = loss_test / len(loader_test.dataset)\n",
    "    accuracy_test   = 100. * float(correct) / len(loader_test.dataset)\n",
    "    return {'loss_test': loss_test, 'accuracy_test': accuracy_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "loss_train_mean = [0 for i in range(epoch)]\n",
    "loss_train_std = [0 for i in range(epoch)]\n",
    "loss_test=[0 for i in range(epoch)]\n",
    "accuracy_test=[0 for i in range(epoch)]\n",
    "for e in range(epoch):\n",
    "    print(\"epoch\",e)\n",
    "        \n",
    "    result_train    = train()\n",
    "    result_test     = test()\n",
    "\n",
    "    loss_train_mean[e]  = result_train['loss_train_mean']\n",
    "    loss_train_std[e]   = result_train['loss_train_std']\n",
    "    loss_test[e]        = result_test['loss_test']\n",
    "    accuracy_test[e]    = result_test['accuracy_test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
